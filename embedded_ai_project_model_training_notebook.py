# -*- coding: utf-8 -*-
"""Embedded AI Project Model Training Notebook

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16boXydr1b210Bo_ulsx6SHbUV7_kQBO8

##### *Copyright 2020 Google LLC*
*Licensed under the Apache License, Version 2.0 (the "License")*
"""

# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""## Import the required libraries
We have the required dataset in a misleadingly named shared drive folder:

We set the path of the dataset here:
"""

from google.colab import drive
drive.mount('/content/drive/')

path = '/content/drive/My Drive/ELEN 6350/6908 Dataset/'

"""Import all the relevant libraries"""

import tensorflow as tf

import os
import numpy as np
import matplotlib.pyplot as plt

from google.colab import drive
drive.mount('/content/drive')

"""## Prepare the training data

We are using a six-class recycling/trash dataset, with each class separated into different folders.
"""

data_dir = os.path.join(path, 'Garbage classification')
# data_dir = '/content/drive/Shareddrives/ELEN 6350/6908 Dataset/Garbage classification'

print(os.getcwd())
os.chdir('/content/drive/My Drive/ELEN 6350/6908 Dataset/')

print("Current directory:", os.getcwd())

print(data_dir)
if os.path.exists(data_dir):
    print("Path exists.")
else:
    print("Path does not exist.")

"""Next, we use [`ImageDataGenerator`](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator) to rescale the image data into float values (divide by 255 so the tensor values are between 0 and 1), and call `flow_from_directory()` to create two generators: one for the training dataset and one for the validation dataset. Since the base model is optimized for 224x244 images, we downscale out images as well to 224x224

"""

IMAGE_SIZE = 224
BATCH_SIZE = 64

datagen = tf.keras.preprocessing.image.ImageDataGenerator(
    rescale=1./255,
    validation_split=0.2)

train_generator = datagen.flow_from_directory(
    data_dir,
    target_size=(IMAGE_SIZE, IMAGE_SIZE),
    batch_size=BATCH_SIZE,
    subset='training')

val_generator = datagen.flow_from_directory(
    data_dir,
    target_size=(IMAGE_SIZE, IMAGE_SIZE),
    batch_size=BATCH_SIZE,
    subset='validation')

"""
On each iteration, these generators provide a batch of images by reading images from disk and processing them to the proper tensor size (224 x 224). The output is a tuple of (images, labels). The shapes are shown here"""

image_batch, label_batch = next(val_generator)
image_batch.shape, label_batch.shape

"""Now save the class labels to a text file:"""

print (train_generator.class_indices)

labels = '\n'.join(sorted(train_generator.class_indices.keys()))

with open('garbage_labels.txt', 'w') as f:
  f.write(labels)

!cat garbage_labels.txt

"""## Building the 6 models

We create 3 models that center around a pre-trained MobileNet V2 model, and 3 customized models wrriten and trained entirely from scratch.

### Create the base model

When instantiating the `MobileNetV2` model, we specify the `include_top=False` argument in order to load the network *without* the classification layers at the top. Then we set `trainable` false to freeze all the weights in the base model. This effectively converts the model into a feature extractor because all the pre-trained weights and biases are preserved in the lower layers when we begin training for our classification head.
"""

IMG_SHAPE = (IMAGE_SIZE, IMAGE_SIZE, 3)

# Define the base model from the pre-trained MobileNet V2
base_model_MNV2 = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,
                                              include_top=False,
                                              weights='imagenet')
base_model_MNV2.trainable = False

"""### Define the models

We create a 3 models with [`Sequential`](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential) model and pass the frozen MobileNet model as the base of the graph, and append additional convolutional, dropout, and dense feedfoward layers so we can set the final output dimension to match the number of classes in our dataset (6 types of classes). We do this for Models A, B and C. For the models D, E, and F, we do this step similarly like in above, but we do not pass in the base pretrained model, but start with an input layer if size (224, 224, 3), and append a series of convolutional + batch norm + pooling layers, before reaching 2 dense feedforward layers at the output.
"""

modelA = tf.keras.Sequential([
  base_model_MNV2,
  tf.keras.layers.Conv2D(filters=32, kernel_size=5, activation='relu'),
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.Flatten(),
  tf.keras.layers.Dense(units=6, activation='softmax')
])

modelB = tf.keras.Sequential([
  base_model_MNV2,
  tf.keras.layers.Conv2D(filters=32, kernel_size=5, activation='relu'),
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.Flatten(),
  tf.keras.layers.Dense(units = 128, activation='relu'),
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.Dense(units = 64, activation='relu'),
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.Dense(units=6, activation='softmax', kernel_regularizer=tf.keras.regularizers.l2(0.1))
])

modelC = tf.keras.Sequential([
  base_model_MNV2,
  tf.keras.layers.Conv2D(filters=32, kernel_size=2, activation='relu'),
  tf.keras.layers.BatchNormalization(),
  tf.keras.layers.MaxPooling2D((2,2)),
  tf.keras.layers.Dropout(0.1),
  tf.keras.layers.Conv2D(filters=64, kernel_size=2, activation='relu'),
  tf.keras.layers.BatchNormalization(),
  tf.keras.layers.MaxPooling2D((2,2)),
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.Flatten(),
  tf.keras.layers.Dense(units = 64, activation='relu'),
  tf.keras.layers.Dropout(0.4),
  tf.keras.layers.Dense(units = 6, activation='softmax', kernel_regularizer=tf.keras.regularizers.l2(0.1))
])

modelD = tf.keras.Sequential([
  tf.keras.layers.InputLayer(shape = (224, 224, 3)),

  tf.keras.layers.Conv2D(filters=16, kernel_size = 3, activation = 'relu'),
  tf.keras.layers.BatchNormalization(),
  tf.keras.layers.MaxPooling2D((2, 2)),
  tf.keras.layers.Dropout(0.2),

  tf.keras.layers.Conv2D(filters=32, kernel_size = 5, activation = 'relu'),
  tf.keras.layers.BatchNormalization(),
  tf.keras.layers.MaxPooling2D((2, 2)),
  tf.keras.layers.Dropout(0.2),

  tf.keras.layers.Conv2D(filters=64, kernel_size = 7, activation = 'relu'),
  tf.keras.layers.BatchNormalization(),
  tf.keras.layers.MaxPooling2D((2, 2)),
  tf.keras.layers.Dropout(0.2),

  tf.keras.layers.Flatten(),
  tf.keras.layers.Dense(128, activation='relu'),
  tf.keras.layers.Dropout(0.3),
  tf.keras.layers.Dense(6, activation='softmax', kernel_regularizer=tf.keras.regularizers.l2(0.1))
])

modelE = tf.keras.Sequential([
  tf.keras.layers.InputLayer(shape = (224, 224, 3)),

  tf.keras.layers.Conv2D(filters=32, kernel_size = 5, activation = 'relu'),
  tf.keras.layers.BatchNormalization(),
  tf.keras.layers.MaxPooling2D((2, 2)),
  tf.keras.layers.Dropout(0.2),

  tf.keras.layers.Conv2D(filters=64, kernel_size = 7, activation = 'relu'),
  tf.keras.layers.BatchNormalization(),
  tf.keras.layers.MaxPooling2D((2, 2)),

  tf.keras.layers.Conv2D(filters=64, kernel_size = 11, activation = 'relu'),
  tf.keras.layers.BatchNormalization(),
  tf.keras.layers.MaxPooling2D((2, 2)),

  tf.keras.layers.Conv2D(filters=64, kernel_size = 15, activation = 'relu'),
  tf.keras.layers.BatchNormalization(),
  tf.keras.layers.MaxPooling2D((2, 2)),
  tf.keras.layers.Dropout(0.2),

  tf.keras.layers.Flatten(),
  tf.keras.layers.Dense(64, activation='relu'),
  tf.keras.layers.Dropout(0.3),
  tf.keras.layers.Dense(6, activation='softmax', kernel_regularizer=tf.keras.regularizers.l2(0.1))
])

modelF = tf.keras.Sequential([
  tf.keras.layers.InputLayer(shape = (224, 224, 3)),

  tf.keras.layers.Conv2D(filters=16, kernel_size = 3, activation = 'relu'),
  tf.keras.layers.BatchNormalization(),
  tf.keras.layers.MaxPooling2D((2, 2)),
  tf.keras.layers.Dropout(0.1),

  tf.keras.layers.Conv2D(filters=32, kernel_size = 3, activation = 'relu'),
  tf.keras.layers.BatchNormalization(),
  tf.keras.layers.MaxPooling2D((2, 2)),
  tf.keras.layers.Dropout(0.1),

  tf.keras.layers.Conv2D(filters=64, kernel_size = 3, activation = 'relu'),
  tf.keras.layers.BatchNormalization(),
  tf.keras.layers.Conv2D(filters=64, kernel_size = 3, activation = 'relu'),
  tf.keras.layers.BatchNormalization(),
  tf.keras.layers.MaxPooling2D((2, 2)),
  tf.keras.layers.Dropout(0.2),

  tf.keras.layers.Conv2D(filters=128, kernel_size = 3, activation = 'relu'),
  tf.keras.layers.BatchNormalization(),
  tf.keras.layers.Conv2D(filters=128, kernel_size = 3, activation = 'relu'),
  tf.keras.layers.BatchNormalization(),
  tf.keras.layers.MaxPooling2D((2, 2)),
  tf.keras.layers.Dropout(0.2),

  tf.keras.layers.Conv2D(filters=128, kernel_size = 5, activation = 'relu'),
  tf.keras.layers.BatchNormalization(),
  tf.keras.layers.Conv2D(filters=128, kernel_size = 5, activation = 'relu'),
  tf.keras.layers.BatchNormalization(),
  tf.keras.layers.MaxPooling2D((2, 2)),
  tf.keras.layers.Dropout(0.2),

  tf.keras.layers.Flatten(),
  tf.keras.layers.Dense(128, activation='relu'),
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.Dense(6, activation='softmax', kernel_regularizer=tf.keras.regularizers.l2(0.1))
])

"""### Configure the model

Although this method is called `compile()`, it's basically a configuration step that's required before we can start training.
"""

modelA.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])
modelB.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])
modelC.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])
modelD.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])
modelE.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])
modelF.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])

"""We display the layers in the model architecture, for all 6 models"""

print("Model A Architecture:")
modelA.summary()
print("Model B Architecture:")
modelB.summary()
print("Model C Architecture:")
modelC.summary()
print("Model D Architecture:")
modelD.summary()
print("Model E Architecture:")
modelE.summary()
print("Model F Architecture:")
modelF.summary()

"""And because the majority of the layers in the models A, B and C is frozen in the base model,  only the last convolution and dense layers are trainable:"""

print('Number of trainable layers in Model A = {}'.format(len(modelA.trainable_weights)))
print('Number of trainable layers in Model B = {}'.format(len(modelB.trainable_weights)))
print('Number of trainable layers in Model C = {}'.format(len(modelC.trainable_weights)))
print('Number of trainable layers in Model D = {}'.format(len(modelD.trainable_weights)))
print('Number of trainable layers in Model E = {}'.format(len(modelE.trainable_weights)))
print('Number of trainable layers in Model F = {}'.format(len(modelF.trainable_weights)))

"""## Train the model

We train the model using data provided by the `train_generator` and `val_generator` during the data preprocessing step
"""

historyA = modelA.fit(train_generator,
                    steps_per_epoch=len(train_generator),
                    epochs=10,
                    validation_data=val_generator,
                    validation_steps=len(val_generator))

historyB = modelB.fit(train_generator,
                    steps_per_epoch=len(train_generator),
                    epochs=10,
                    validation_data=val_generator,
                    validation_steps=len(val_generator))

historyC = modelC.fit(train_generator,
                    steps_per_epoch=len(train_generator),
                    epochs=10,
                    validation_data=val_generator,
                    validation_steps=len(val_generator))

historyD = modelD.fit(train_generator,
                    steps_per_epoch=len(train_generator),
                    epochs=25,
                    validation_data=val_generator,
                    validation_steps=len(val_generator))

historyE = modelE.fit(train_generator,
                    steps_per_epoch=len(train_generator),
                    epochs=25,
                    validation_data=val_generator,
                    validation_steps=len(val_generator))

historyF = modelF.fit(train_generator,
                    steps_per_epoch=len(train_generator),
                    epochs=25,
                    validation_data=val_generator,
                    validation_steps=len(val_generator))

"""### Training Curves

"""

acc_A = historyA.history['accuracy']
val_acc_A = historyA.history['val_accuracy']

loss_A = historyA.history['loss']
val_loss_A = historyA.history['val_loss']

acc_B = historyB.history['accuracy']
val_acc_B = historyB.history['val_accuracy']

loss_B = historyB.history['loss']
val_loss_B = historyB.history['val_loss']

acc_C = historyC.history['accuracy']
val_acc_C = historyC.history['val_accuracy']

loss_C = historyC.history['loss']
val_loss_C = historyC.history['val_loss']

acc_D = historyD.history['accuracy']
val_acc_D = historyD.history['val_accuracy']

loss_D = historyD.history['loss']
val_loss_D = historyD.history['val_loss']

acc_E = historyE.history['accuracy']
val_acc_E = historyE.history['val_accuracy']

loss_E = historyE.history['loss']
val_loss_E = historyE.history['val_loss']

acc_F = historyF.history['accuracy']
val_acc_F = historyF.history['val_accuracy']

loss_F = historyF.history['loss']
val_loss_F = historyF.history['val_loss']

plt.figure(figsize=(16, 10))
plt.subplot(2, 2, 1)
plt.plot(acc_A, label='Training Accuracy of Model A')
plt.plot(acc_B, label='Training Accuracy of Model B')
plt.plot(acc_C, label='Training Accuracy of Model C')
plt.legend(loc='lower right')
plt.ylabel('Accuracy')
plt.xlabel('epoch')
plt.ylim([min(plt.ylim()),1])
plt.title('Training Accuracy of Models A-C')

plt.subplot(2, 2, 2)
plt.plot(val_acc_A, label='Validation Accuracy of Model A')
plt.plot(val_acc_B, label='Validation Accuracy of Model B')
plt.plot(val_acc_C, label='Validation Accuracy of Model C')
plt.legend(loc='lower right')
plt.ylabel('Accuracy')
plt.xlabel('epoch')
plt.ylim([min(plt.ylim()),1])
plt.title('Validation Accuracy of Models A-C')

plt.subplot(2, 2, 3)
plt.plot(loss_A, label='Training Loss of Model A')
plt.plot(loss_B, label='Training Loss of Model B')
plt.plot(loss_C, label='Training Loss of Model C')
plt.legend(loc='upper right')
plt.ylabel('Cross Entropy')
plt.ylim([0,max(plt.ylim())])
plt.title('Training Loss of Models A-C')
plt.xlabel('epoch')

plt.subplot(2, 2, 4)
plt.plot(val_loss_A, label='Validation Loss of Model A')
plt.plot(val_loss_B, label='Validation Loss of Model B')
plt.plot(val_loss_C, label='Validation Loss of Model C')
plt.legend(loc='upper right')
plt.ylabel('Cross Entropy')
plt.ylim([0,max(plt.ylim())])
plt.title('Validation Loss of Models A-C')
plt.xlabel('epoch')
plt.show()

plt.figure(figsize=(16, 10))
plt.subplot(2, 2, 1)
plt.plot(acc_D, label='Training Accuracy of Model D')
plt.plot(acc_E, label='Training Accuracy of Model E')
plt.plot(acc_F, label='Training Accuracy of Model F')
plt.legend(loc='lower right')
plt.ylabel('Accuracy')
plt.xlabel('epoch')
plt.ylim([min(plt.ylim()),1])
plt.title('Training Accuracy of Models D-F')

plt.subplot(2, 2, 2)
plt.plot(val_acc_D, label='Validation Accuracy of Model D')
plt.plot(val_acc_E, label='Validation Accuracy of Model E')
plt.plot(val_acc_F, label='Validation Accuracy of Model F')
plt.legend(loc='lower right')
plt.ylabel('Accuracy')
plt.xlabel('epoch')
plt.ylim([min(plt.ylim()),1])
plt.title('Validation Accuracy of Models D-F')

plt.subplot(2, 2, 3)
plt.plot(loss_D, label='Training Loss of Model D')
plt.plot(loss_E, label='Training Loss of Model E')
plt.plot(loss_F, label='Training Loss of Model F')
plt.legend(loc='upper right')
plt.ylabel('Cross Entropy')
plt.ylim([0,max(plt.ylim())])
plt.title('Training Loss of Models D-F')
plt.xlabel('epoch')

plt.subplot(2, 2, 4)
plt.plot(val_loss_D, label='Validation Loss of Model D')
plt.plot(val_loss_E, label='Validation Loss of Model E')
plt.plot(val_loss_F, label='Validation Loss of Model F')
plt.legend(loc='upper right')
plt.ylabel('Cross Entropy')
plt.ylim([0,max(plt.ylim())])
plt.title('Validation Loss of Models D-F')
plt.xlabel('epoch')
plt.show()

"""## Fine tune the base model for Models A, B and C

After training classification layers, we wanted to study if training the weights of the pre-trained network weren't initially changed could further boost classification accuracy

### Un-freeze more layers

We first check many layers are in the base model
"""

print("Number of layers in the base model: ", len(base_model_MNV2.layers))

"""Let's try freezing just the bottom 50 layers."""

base_model_MNV2.trainable = True
fine_tune_at = 50

# Freeze all the layers before the `fine_tune_at` layer
for layer in base_model_MNV2.layers[:fine_tune_at]:
  layer.trainable =  False

"""### Reconfigure the model

Now configure the model again, but this time with a lower learning rate (the default is 0.001).
"""

modelA.compile(optimizer=tf.keras.optimizers.Adam(1e-5),
              loss='categorical_crossentropy',
              metrics=['accuracy'])
modelB.compile(optimizer=tf.keras.optimizers.Adam(1e-5),
              loss='categorical_crossentropy',
              metrics=['accuracy'])
modelC.compile(optimizer=tf.keras.optimizers.Adam(1e-5),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

print("Model A Architecture:")
modelA.summary()
print("Model B Architecture:")
modelB.summary()
print("Model C Architecture:")
modelC.summary()
print("Model D Architecture:")
modelD.summary()
print("Model E Architecture:")
modelE.summary()
print("Model F Architecture:")
modelF.summary()

print('Number of trainable layers = {}'.format(len(modelA.trainable_weights)))
print('Number of trainable layers = {}'.format(len(modelB.trainable_weights)))
print('Number of trainable layers = {}'.format(len(modelC.trainable_weights)))
print('Number of trainable layers = {}'.format(len(modelD.trainable_weights)))
print('Number of trainable layers = {}'.format(len(modelE.trainable_weights)))
print('Number of trainable layers = {}'.format(len(modelF.trainable_weights)))

"""### Continue training

In our 2nd training run, we fine-tune all trainable layers, including the lower layers of the base model. Since we are starting with the weights already trained in the classification layers during the first training run, we reduce the amount of epochs to 5
"""

history_fine_A = modelA.fit(train_generator,
                         steps_per_epoch=len(train_generator),
                         epochs=5,
                         validation_data=val_generator,
                         validation_steps=len(val_generator))

history_fine_B = modelB.fit(train_generator,
                         steps_per_epoch=len(train_generator),
                         epochs=5,
                         validation_data=val_generator,
                         validation_steps=len(val_generator))

history_fine_C = modelC.fit(train_generator,
                         steps_per_epoch=len(train_generator),
                         epochs=5,
                         validation_data=val_generator,
                         validation_steps=len(val_generator))

"""### Review the new learning curves"""

acc_A = history_fine_A.history['accuracy']
val_acc_A = history_fine_A.history['val_accuracy']

loss_A = history_fine_A.history['loss']
val_loss_A = history_fine_A.history['val_loss']

acc_B = history_fine_B.history['accuracy']
val_acc_B = history_fine_B.history['val_accuracy']

loss_B = history_fine_B.history['loss']
val_loss_B = history_fine_B.history['val_loss']

acc_C = history_fine_C.history['accuracy']
val_acc_C = history_fine_C.history['val_accuracy']

loss_C = history_fine_C.history['loss']
val_loss_C = history_fine_C.history['val_loss']

plt.figure(figsize=(16, 10))
plt.subplot(2, 2, 1)
plt.plot(acc_A, label='Training Accuracy of Model A')
plt.plot(acc_B, label='Training Accuracy of Model B')
plt.plot(acc_C, label='Training Accuracy of Model C')
plt.legend(loc='lower right')
plt.ylabel('Accuracy')
plt.xlabel('epoch')
plt.ylim([min(plt.ylim()),1])
plt.title('Training Accuracy')

plt.subplot(2, 2, 2)
plt.plot(val_acc_A, label='Validation Accuracy of Model A')
plt.plot(val_acc_B, label='Validation Accuracy of Model B')
plt.plot(val_acc_C, label='Validation Accuracy of Model C')
plt.legend(loc='upper right')
plt.ylabel('Accuracy')
plt.xlabel('epoch')
plt.ylim([min(plt.ylim()),1])
plt.title('Validation Accuracy')

plt.subplot(2, 2, 3)
plt.plot(loss_A, label='Training Loss of Model A')
plt.plot(loss_B, label='Training Loss of Model B')
plt.plot(loss_C, label='Training Loss of Model C')
plt.legend(loc='upper right')
plt.ylabel('Cross Entropy')
plt.ylim([0,2.0])
plt.title('Training Loss')
plt.xlabel('epoch')

plt.subplot(2, 2, 4)
plt.plot(val_loss_A, label='Validation Loss of Model A')
plt.plot(val_loss_B, label='Validation Loss of Model B')
plt.plot(val_loss_C, label='Validation Loss of Model C')
plt.legend(loc='upper right')
plt.ylabel('Cross Entropy')
plt.ylim([0,2.0])
plt.title('Validation Loss')
plt.xlabel('epoch')
plt.show()

"""As shown, validation loss is still greater than training loss, which might indicate some overfitting. More importantly, it seems that fine-tuning does not improve validation accuracy by much if at all, indicating the base pre-trained model already generalized well to different classes of garbage image data.

## Convert to TFLite

We need to perform post-training quantization of the model to int8 format, we need to perform [post-training quantization](https://www.tensorflow.org/lite/performance/post_training_quantization) with a representative dataset, which requires a few more arguments for the `TFLiteConverter`, and a function that builds a dataset that's representative of the training dataset. This function is the same one used in 'Retrain MobileNet V2 classifier for the Edge TPU (TF2)' Google Colab Notebook Tutorial provided by Google Coral.
"""

# A generator that provides a representative dataset
def representative_data_gen():
  dataset_list = tf.data.Dataset.list_files(data_dir + '/*/*')
  for i in range(100):
    image = next(iter(dataset_list))
    image = tf.io.read_file(image)
    image = tf.io.decode_jpeg(image, channels=3)
    image = tf.image.resize(image, [IMAGE_SIZE, IMAGE_SIZE])
    image = tf.cast(image / 255., tf.float32)
    image = tf.expand_dims(image, 0)
    yield [image]

converter_A = tf.lite.TFLiteConverter.from_keras_model(modelA)
converter_B = tf.lite.TFLiteConverter.from_keras_model(modelB)
converter_C = tf.lite.TFLiteConverter.from_keras_model(modelC)
converter_D = tf.lite.TFLiteConverter.from_keras_model(modelD)
converter_E = tf.lite.TFLiteConverter.from_keras_model(modelE)
converter_F = tf.lite.TFLiteConverter.from_keras_model(modelF)

# This enables quantization
converter_A.optimizations = [tf.lite.Optimize.DEFAULT]
converter_B.optimizations = [tf.lite.Optimize.DEFAULT]
converter_C.optimizations = [tf.lite.Optimize.DEFAULT]
converter_D.optimizations = [tf.lite.Optimize.DEFAULT]
converter_E.optimizations = [tf.lite.Optimize.DEFAULT]
converter_F.optimizations = [tf.lite.Optimize.DEFAULT]

# This sets the representative dataset for quantization
converter_A.representative_dataset = representative_data_gen
converter_B.representative_dataset = representative_data_gen
converter_C.representative_dataset = representative_data_gen
converter_D.representative_dataset = representative_data_gen
converter_E.representative_dataset = representative_data_gen
converter_F.representative_dataset = representative_data_gen

# This ensures that if any ops can't be quantized, the converter throws an error
converter_A.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
converter_B.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
converter_C.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
converter_D.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
converter_E.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
converter_F.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]

# For full integer quantization, though supported types defaults to int8 only, we explicitly declare it for clarity.
converter_A.target_spec.supported_types = [tf.int8]
converter_B.target_spec.supported_types = [tf.int8]
converter_C.target_spec.supported_types = [tf.int8]
converter_D.target_spec.supported_types = [tf.int8]
converter_E.target_spec.supported_types = [tf.int8]
converter_F.target_spec.supported_types = [tf.int8]
# These set the input and output tensors to uint8 (added in r2.3)
converter_A.inference_input_type = tf.uint8
converter_A.inference_output_type = tf.uint8

converter_B.inference_input_type = tf.uint8
converter_B.inference_output_type = tf.uint8

converter_C.inference_input_type = tf.uint8
converter_C.inference_output_type = tf.uint8

converter_D.inference_input_type = tf.uint8
converter_D.inference_output_type = tf.uint8

converter_E.inference_input_type = tf.uint8
converter_E.inference_output_type = tf.uint8

converter_F.inference_input_type = tf.uint8
converter_F.inference_output_type = tf.uint8

tflite_model_A = converter_A.convert()
tflite_model_B = converter_B.convert()
tflite_model_C = converter_C.convert()
tflite_model_D = converter_D.convert()
tflite_model_E = converter_E.convert()
tflite_model_F = converter_F.convert()

with open('garbage_model_A.tflite', 'wb') as f:
  f.write(tflite_model_A)
with open('garbage_model_B.tflite', 'wb') as f:
  f.write(tflite_model_B)
with open('garbage_model_C.tflite', 'wb') as f:
  f.write(tflite_model_C)
with open('garbage_model_D.tflite', 'wb') as f:
  f.write(tflite_model_D)
with open('garbage_model_E.tflite', 'wb') as f:
  f.write(tflite_model_E)
with open('garbage_model_F.tflite', 'wb') as f:
  f.write(tflite_model_F)

"""### Compare the accuracy

After fully quantizing the 6 TensorFlow Lite models, we evaluate both the raw model and the TensorFlow Lite model to ensure minimala accuracy drop pre and post quantization

The test accuracy of the raw models are shown below, for a randomly selected test data batch from the dataset
"""

batch_images, batch_labels = next(val_generator)

logitsA = modelA(batch_images)
predictionA = np.argmax(logitsA, axis=1)
logitsB = modelB(batch_images)
predictionB = np.argmax(logitsB, axis=1)
logitsC = modelC(batch_images)
predictionC = np.argmax(logitsC, axis=1)
logitsD = modelD(batch_images)
predictionD = np.argmax(logitsD, axis=1)
logitsE = modelE(batch_images)
predictionE = np.argmax(logitsE, axis=1)
logitsF = modelF(batch_images)
predictionF = np.argmax(logitsF, axis=1)

truth = np.argmax(batch_labels, axis=1)

keras_accuracy_A = tf.keras.metrics.Accuracy()
keras_accuracy_A(predictionA, truth)
keras_accuracy_B = tf.keras.metrics.Accuracy()
keras_accuracy_B(predictionB, truth)
keras_accuracy_C = tf.keras.metrics.Accuracy()
keras_accuracy_C(predictionC, truth)
keras_accuracy_D = tf.keras.metrics.Accuracy()
keras_accuracy_D(predictionD, truth)
keras_accuracy_E = tf.keras.metrics.Accuracy()
keras_accuracy_E(predictionE, truth)
keras_accuracy_F = tf.keras.metrics.Accuracy()
keras_accuracy_F(predictionF, truth)

print("Raw model A accuracy: {:.3%}".format(keras_accuracy_A.result()))
print("Raw model B accuracy: {:.3%}".format(keras_accuracy_B.result()))
print("Raw model C accuracy: {:.3%}".format(keras_accuracy_C.result()))
print("Raw model D accuracy: {:.3%}".format(keras_accuracy_D.result()))
print("Raw model E accuracy: {:.3%}".format(keras_accuracy_E.result()))
print("Raw model F accuracy: {:.3%}".format(keras_accuracy_F.result()))

"""We now check the accuracy of the `.tflite` quantized model, using the test data batch

Since there was no convenient API to evaluate the accuracy of a TensorFlow Lite model, in the 'Retrain MobileNet V2 classifier for the Edge TPU (TF2)' tutorial Colab Notebook provided by Google Coral, set_input_tensor() and classify_image() functions were provided to run several inferences and compare the predictions against ground truth:
"""

def set_input_tensor(interpreter, input):
  input_details = interpreter.get_input_details()[0]
  tensor_index = input_details['index']
  input_tensor = interpreter.tensor(tensor_index)()[0]
  # Inputs for the TFLite model must be uint8, so we quantize our input data.
  # NOTE: This step is necessary only because we're receiving input data from
  # ImageDataGenerator, which rescaled all image data to float [0,1]. When using
  # bitmap inputs, they're already uint8 [0,255] so this can be replaced with:
  #   input_tensor[:, :] = input
  scale, zero_point = input_details['quantization']
  input_tensor[:, :] = np.uint8(input / scale + zero_point)

def classify_image(interpreter, input):
  set_input_tensor(interpreter, input)
  interpreter.invoke()
  output_details = interpreter.get_output_details()[0]
  output = interpreter.get_tensor(output_details['index'])
  # Outputs from the TFLite model are uint8, so we dequantize the results:
  scale, zero_point = output_details['quantization']
  output = scale * (output - zero_point)
  top_1 = np.argmax(output)
  return top_1

interpreterA = tf.lite.Interpreter('garbage_model_A.tflite')
interpreterA.allocate_tensors()
interpreterB = tf.lite.Interpreter('garbage_model_B.tflite')
interpreterB.allocate_tensors()
interpreterC = tf.lite.Interpreter('garbage_model_C.tflite')
interpreterC.allocate_tensors()
interpreterD = tf.lite.Interpreter('garbage_model_D.tflite')
interpreterD.allocate_tensors()
interpreterE = tf.lite.Interpreter('garbage_model_E.tflite')
interpreterE.allocate_tensors()
interpreterF = tf.lite.Interpreter('garbage_model_E.tflite')
interpreterF.allocate_tensors()
# Collect all inference predictions in a list
batch_prediction_A = []
batch_prediction_B = []
batch_prediction_C = []
batch_prediction_D = []
batch_prediction_E = []
batch_prediction_F = []
batch_truth = np.argmax(batch_labels, axis=1)

for i in range(len(batch_images)):
  prediction_A = classify_image(interpreterA, batch_images[i])
  prediction_B = classify_image(interpreterB, batch_images[i])
  prediction_C = classify_image(interpreterC, batch_images[i])
  prediction_D = classify_image(interpreterD, batch_images[i])
  prediction_E = classify_image(interpreterE, batch_images[i])
  prediction_F = classify_image(interpreterF, batch_images[i])
  batch_prediction_A.append(prediction_A)
  batch_prediction_B.append(prediction_B)
  batch_prediction_C.append(prediction_C)
  batch_prediction_D.append(prediction_D)
  batch_prediction_E.append(prediction_E)
  batch_prediction_F.append(prediction_F)

# Compare all predictions to the ground truth
tflite_model_A_accuracy = tf.keras.metrics.Accuracy()
tflite_model_B_accuracy = tf.keras.metrics.Accuracy()
tflite_model_C_accuracy = tf.keras.metrics.Accuracy()
tflite_model_D_accuracy = tf.keras.metrics.Accuracy()
tflite_model_E_accuracy = tf.keras.metrics.Accuracy()
tflite_model_F_accuracy = tf.keras.metrics.Accuracy()

tflite_model_A_accuracy(batch_prediction_A, batch_truth)
tflite_model_B_accuracy(batch_prediction_B, batch_truth)
tflite_model_C_accuracy(batch_prediction_C, batch_truth)
tflite_model_D_accuracy(batch_prediction_D, batch_truth)
tflite_model_E_accuracy(batch_prediction_E, batch_truth)
tflite_model_F_accuracy(batch_prediction_F, batch_truth)

print("Quantized Model A TF Lite accuracy: {:.3%}".format(tflite_model_A_accuracy.result()))
print("Quantized Model B TF Lite accuracy: {:.3%}".format(tflite_model_B_accuracy.result()))
print("Quantized Model C TF Lite accuracy: {:.3%}".format(tflite_model_C_accuracy.result()))
print("Quantized Model D TF Lite accuracy: {:.3%}".format(tflite_model_D_accuracy.result()))
print("Quantized Model E TF Lite accuracy: {:.3%}".format(tflite_model_E_accuracy.result()))
print("Quantized Model F TF Lite accuracy: {:.3%}".format(tflite_model_F_accuracy.result()))

"""As shown, there is typically little to moderate difference in the testing accuracy between pre- and post-quantization models

## Compile for the Edge TPU

We compile the models for the Edge TPU., first by downloading the [Edge TPU Compiler](https://coral.ai/docs/edgetpu/compiler/):
"""

! curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -

! echo "deb https://packages.cloud.google.com/apt coral-edgetpu-stable main" | sudo tee /etc/apt/sources.list.d/coral-edgetpu.list

! sudo apt-get update

! sudo apt-get install edgetpu-compiler

"""Then we check the workign directory to ensure correct paths"""

cwd = os.getcwd()
print("Current working directory:", cwd)
os.chdir('/content/drive/My Drive/ELEN 6350/6908 Dataset')
print("Current working directory:", os.getcwd())

"""We then compile the models one-by-one"""

! edgetpu_compiler 'garbage_model_A.tflite'

! edgetpu_compiler 'garbage_model_B.tflite'

! edgetpu_compiler 'garbage_model_C.tflite'

! edgetpu_compiler 'garbage_model_D.tflite'

! edgetpu_compiler 'garbage_model_E.tflite'

! edgetpu_compiler 'garbage_model_F.tflite'

"""All models except for Model E were sucessfully compiled. We could not debug why compilation failed for Model E, so it is skipped for the actualy model testing on the device

## Download the model

Dowwnload the 5 successfuly compiled TFLite models
"""

from google.colab import files

files.download('garbage_model_A_edgetpu.tflite')
files.download('garbage_model_B_edgetpu.tflite')
files.download('garbage_model_C_edgetpu.tflite')
files.download('garbage_model_D_edgetpu.tflite')
files.download('garbage_model_F_edgetpu.tflite')
files.download('garbage_labels.txt')

"""## Run the model on the Edge TPU

You can now run the model on your Coral device with acceleration on the Edge TPU.

To get started, try using your `.tflite` model with [this code for image classification with the TensorFlow Lite API](https://github.com/google-coral/tflite/tree/master/python/examples/classification).

Just follow the instructions on that page to set up your device, copy the `mobilenet_v2_1.0_224_quant_edgetpu.tflite` and `flower_labels.txt` files to your Coral Dev Board or device with a Coral Accelerator, and pass it a flower photo like this:

```
python3 classify_image.py \
  --model mobilenet_v2_1.0_224_quant_edgetpu.tflite \
  --labels flower_labels.txt \
  --input flower.jpg
```

Check out more examples for running inference at [coral.ai/examples](https://coral.ai/examples/#code-examples/).
"""